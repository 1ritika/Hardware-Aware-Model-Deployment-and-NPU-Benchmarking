# âš™ï¸ Hardware-Aware Model Deployment and NPU Benchmarking

This project focuses on **deploying and benchmarking deep learning models** (CNN and Transformer) across different hardware backends â€” **CPU**, **AMD GPU**, and **NPU** â€” using **ONNX Runtime**. The goal is to analyze performance trade-offs in precision formats (FP32, FP16, INT8) for edge-efficient inference.

---

## ğŸš€ Overview
- Converted PyTorch-trained CIFAR-10 models (CNN and Transformer) into **ONNX format** for cross-device deployment.  
- Evaluated inference latency, throughput, and accuracy across heterogeneous compute devices.  
- Benchmarked model performance under **mixed precision (FP32 / FP16 / INT8)** configurations.  
- Compared **NPU acceleration efficiency** against CPU and GPU baselines.  

---

## ğŸ“ˆ Key Results
| Hardware | Precision | Avg. Inference Speed-up | Notes |
|-----------|------------|--------------------------|--------|
| CPU | FP32 | 1Ã— (baseline) | Consistent accuracy |
| AMD GPU | FP16 | **2.8Ã— faster** | Minor accuracy drop |
| NPU | INT8 | **4.5Ã— faster** | Best throughput, reduced energy cost |

> ONNXRuntimeâ€™s hardware-agnostic execution enables efficient deployment across devices with minimal code changes.

---

## ğŸ§  Technical Details
- **Models:** CNN and Transformer classifiers on CIFAR-10.  
- **Frameworks:** PyTorch â†’ ONNX conversion â†’ ONNX Runtime execution.  
- **Metrics:** Inference latency, speed-up factor, and precisionâ€“accuracy trade-off.  
- **Profiling Tools:** ONNX Runtime Profiler and PyTorch CUDA events.

---

## ğŸ› ï¸ Requirements
```bash
pip install torch torchvision onnx onnxruntime numpy matplotlib
